{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Checking for the device- i.e if torch is using CUDA or not.\n",
    "# If it is not using CUDA, it will train on CPU.\n",
    "# Else it will train on GPU.\n",
    "\n",
    "# 'cuda' and 'cpu' have to be as given since they are library terms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing- Using transforms\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "    # Resizing every image to (150, 150) for training\n",
    "    transforms.Resize((150, 150)),\n",
    "    \n",
    "    # Random horizontally flipping images with 0.5 probability\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    \n",
    "    # Changes [0-255] value to [0-1], also changes data type from numpy to tensor\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Changes range from [0-1] to [-1, 1]\n",
    "    # Column represents RGB channel, and row represents mean and std\n",
    "    # x will be replaces by the Z-score = (x-mean)/std\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                        [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "# Paths\n",
    "train_path = '/home/siddhant/Pictures/Intel-image-dataset/archive/seg_train'\n",
    "test_path = '/home/siddhant/Pictures/Intel-image-dataset/archive/seg_test'\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path, transform = transformer),\n",
    "    # Hyper parameters\n",
    "    batch_size = 256,\n",
    "    # Shuffle to get rid of bias\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path, transform = transformer),\n",
    "    # Hyper parameters\n",
    "    batch_size = 256,\n",
    "    # Shuffle to get rid of bias\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n"
     ]
    }
   ],
   "source": [
    "# Getting categories\n",
    "\n",
    "root = pathlib.Path(train_path)\n",
    "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Network- In PyTorch we need to define class for a CNN- it will inherit the nn.Module class\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes = 6):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # Input shape (256, 3, 150, 150) - (batch size, number of channels, height, width)\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 12, kernel_size = 3, stride = 1, padding = 1)\n",
    "        # ((w-f+2p)/s)+1\n",
    "        # New shape = (256, 12, 150, 150)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = 12)\n",
    "        # New shape = (256, 12, 150, 150)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # New shape = (256, 12, 150, 150)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2)\n",
    "        # New shape = (256, 12, 75, 75)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 12, out_channels = 20, kernel_size = 3, stride = 1, padding = 1)\n",
    "        # New shape = (256, 20, 75, 75)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 20, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n",
    "        # New shape = (256, 32, 75, 75)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features = 32)\n",
    "        # New shape = (256, 32, 75, 75)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # New shape = (256, 32, 75, 75)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features = 32*75*75, out_features = num_classes)\n",
    "    \n",
    "    # Feed forward function\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        \n",
    "        output = self.pool(output)\n",
    "        \n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        \n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        \n",
    "        # Output will be [256, 32, 75, 75]\n",
    "        \n",
    "        output = output.view(-1, 32*75*75)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting model and sending it to the device\n",
    "model = ConvNet(num_classes = 6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr = 0.001, weight_decay = 0.0001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the number of training and testing datasets\n",
    "\n",
    "train_count = len(glob.glob(train_path + '/**/*.jpg'))\n",
    "test_count = len(glob.glob(test_path + '/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13307 3000\n"
     ]
    }
   ],
   "source": [
    "print(train_count, test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(13.5074) 0.5319756519125273 0.4673333333333333\n",
      "1 tensor(1.5570) 0.6908394078304652 0.627\n",
      "2 tensor(0.9131) 0.7705718794619373 0.6333333333333333\n",
      "3 tensor(0.7300) 0.8131810325392651 0.7056666666666667\n",
      "4 tensor(0.7723) 0.8241527015856316 0.7166666666666667\n",
      "5 tensor(0.4566) 0.8790110468174644 0.68\n",
      "6 tensor(0.3804) 0.9013301270008266 0.727\n",
      "7 tensor(0.3867) 0.8962951829863981 0.691\n",
      "8 tensor(0.1891) 0.9449913579319156 0.752\n",
      "9 tensor(0.1038) 0.9700157811678064 0.7236666666666667\n"
     ]
    }
   ],
   "source": [
    "# Training model and saving best model\n",
    "\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model.train()\n",
    "    train_accuracy = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cude())\n",
    "            labels = Variable(labels.cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.cpu().data*images.size(0)\n",
    "        _,prediction = torch.max(outputs.data, 1)\n",
    "        \n",
    "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "    \n",
    "    train_accuracy /= train_count\n",
    "    train_loss /= train_count\n",
    "    \n",
    "    # Test and evaluate\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cude())\n",
    "            labels = Variable(labels.cuda())\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _,prediction = torch.max(outputs.data, 1)\n",
    "        \n",
    "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
    "    \n",
    "    test_accuracy /= test_count\n",
    "    \n",
    "    print(epoch, train_loss, train_accuracy, test_accuracy)\n",
    "    \n",
    "    if test_accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_checkpoint.model')\n",
    "        best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
